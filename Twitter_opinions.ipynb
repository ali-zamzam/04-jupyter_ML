{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "608d8726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rechauffementClimatique.csv\n",
      "tweet.csv\n",
      "tweet1.csv\n",
      "modele_twitter.mod\n",
      "rechauffementClimatique_non_preparees.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "#chargement des stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#fonction de normalisation\n",
    "import re\n",
    "\n",
    "#stemmatisation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#jeux d'apprentissageet de test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# desactivation du nombres de colonnes\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "# recuperation des fichiers\n",
    "\n",
    "listDeFichiers = os.listdir(\"data\")\n",
    "\n",
    "#quel est les noms des fichiers\n",
    "\n",
    "for fichier in listDeFichiers:\n",
    "    print(fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7840763",
   "metadata": {},
   "outputs": [],
   "source": [
    "messagesTwitter = pd.read_csv('data/rechauffementClimatique.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a194f6e",
   "metadata": {},
   "source": [
    "### informations sur le nombres d'observations et leur contenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d89a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4225 entries, 0 to 4224\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   TWEET      4225 non-null   object \n",
      " 1   CROYANCE   4225 non-null   object \n",
      " 2   CONFIENCE  4225 non-null   float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 99.1+ KB\n"
     ]
    }
   ],
   "source": [
    "messagesTwitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200f6cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4225, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messagesTwitter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e533cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TWEET</th>\n",
       "      <th>CROYANCE</th>\n",
       "      <th>CONFIENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global warming report urges governments to act...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fighting poverty and global warming in Africa ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.8786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TWEET CROYANCE  CONFIENCE\n",
       "0  Global warming report urges governments to act...      Yes     1.0000\n",
       "1  Fighting poverty and global warming in Africa ...      Yes     1.0000\n",
       "2  Carbon offsets: How a Vatican forest failed to...      Yes     0.8786"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messagesTwitter.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d5f6144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TWEET</th>\n",
       "      <th>CROYANCE</th>\n",
       "      <th>CONFIENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global warming report urges governments to act...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fighting poverty and global warming in Africa ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>URUGUAY: Tools Needed for Those Most Vulnerabl...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4220</th>\n",
       "      <td>It's 83•_Á and climbing in NYC. August weather...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>@bloodless_coup \"The phrase 'global warming' s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222</th>\n",
       "      <td>Global warming you tube parody you will enjoy ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4223</th>\n",
       "      <td>One-Eyed Golfer: Don't dare tell me about glob...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4224</th>\n",
       "      <td>man made global warming a hair brained theory ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TWEET  CROYANCE  CONFIENCE\n",
       "0     Global warming report urges governments to act...         1     1.0000\n",
       "1     Fighting poverty and global warming in Africa ...         1     1.0000\n",
       "2     Carbon offsets: How a Vatican forest failed to...         1     0.8786\n",
       "3     Carbon offsets: How a Vatican forest failed to...         1     1.0000\n",
       "4     URUGUAY: Tools Needed for Those Most Vulnerabl...         1     0.8087\n",
       "...                                                 ...       ...        ...\n",
       "4220  It's 83•_Á and climbing in NYC. August weather...         1     1.0000\n",
       "4221  @bloodless_coup \"The phrase 'global warming' s...         1     1.0000\n",
       "4222  Global warming you tube parody you will enjoy ...         0     0.6411\n",
       "4223  One-Eyed Golfer: Don't dare tell me about glob...         0     1.0000\n",
       "4224  man made global warming a hair brained theory ...         0     1.0000\n",
       "\n",
       "[4225 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messagesTwitter.CROYANCE= (messagesTwitter['CROYANCE']=='Yes').astype(\"int64\")\n",
    "messagesTwitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d5974e",
   "metadata": {},
   "source": [
    "# Normalisation\n",
    "<h1> Module Re (Regular expression)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d742e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisation(message):\n",
    "    message = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', message)\n",
    "    message = re.sub('@[^\\s]+','USER', message)\n",
    "    message = message.lower().replace(\"ё\", \"е\")\n",
    "    message = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', message)\n",
    "    message = re.sub(' +',' ', message)\n",
    "    return message.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5223c918",
   "metadata": {},
   "source": [
    "### text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "014e41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(re)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2fa2f2",
   "metadata": {},
   "source": [
    "## Réaliser la normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f068f683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TWEET</th>\n",
       "      <th>CROYANCE</th>\n",
       "      <th>CONFIENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>global warming report urges governments to act...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fighting poverty and global warming in africa ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>carbon offsets how a vatican forest failed to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TWEET  CROYANCE  CONFIENCE\n",
       "0  global warming report urges governments to act...         1     1.0000\n",
       "1  fighting poverty and global warming in africa ...         1     1.0000\n",
       "2  carbon offsets how a vatican forest failed to ...         1     0.8786"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messagesTwitter.TWEET= messagesTwitter.TWEET.apply(normalisation)\n",
    "messagesTwitter.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fbfe13",
   "metadata": {},
   "source": [
    "<h2>Étape 2 : suppression des stops words</h2>\n",
    "\n",
    "Pour supprimer les stop words de nos messages en anglais, nous allons utiliser ceux connus par le module nltk.corpus : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "591667ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efc921f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "910528c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59449b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "020a7aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  TWEET  CROYANCE  CONFIENCE\n",
      "0     global warming report urges governments act br...         1     1.0000\n",
      "1           fighting poverty global warming africa link         1     1.0000\n",
      "2     carbon offsets vatican forest failed reduce gl...         1     0.8786\n",
      "3     carbon offsets vatican forest failed reduce gl...         1     1.0000\n",
      "4     uruguay tools needed vulnerable climate change...         1     0.8087\n",
      "...                                                 ...       ...        ...\n",
      "4220  83 climbing nyc august weather first day may u...         1     1.0000\n",
      "4221  user phrase global warming abandoned favor cli...         1     1.0000\n",
      "4222     global warming tube parody enjoy ipcc ocra url         0     0.6411\n",
      "4223  one eyed golfer dare tell global warming twent...         0     1.0000\n",
      "4224  man made global warming hair brained theory 4 ...         0     1.0000\n",
      "\n",
      "[4225 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#une fonction lambda qui, pour chaque mot contenu dans les messages\n",
    "#vérifie s’il est connu par la variable stopWords.Cette fonction lambda crée une chaîne vide. \n",
    "#Si le premier mot du message n’est pas connu de la variable stopWords, il est alors gardé et concaténé (join) à la chaîne vide.\n",
    "#Dans le cas contraire, il est ignoré. L’analyse du mot suivant peut alors commencer.\n",
    "#Si le second mot du message n’est pas connu de la variable stopWords, \n",
    "#il est alors gardé et concaténé (join) à la chaîne contenant le premier mot. \n",
    "#L’analyse du troisième mot peut alors commencer, etc.\n",
    "\n",
    "\n",
    "messagesTwitter['TWEET'] = messagesTwitter['TWEET'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopword)]))\n",
    "print(messagesTwitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5929e55",
   "metadata": {},
   "source": [
    "<h2>Étape 3 : le stemming</h2>\n",
    "<p>La stemmisation des messages se fait à l'aide du module nltk et de la classe <b>SnowballStemmer</b>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8226dcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TWEET</th>\n",
       "      <th>CROYANCE</th>\n",
       "      <th>CONFIENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>global warm report urg govern act brussel belg...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fight poverti global warm africa link</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>carbon offset vatican forest fail reduc global...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>carbon offset vatican forest fail reduc global...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uruguay tool need vulner climat chang link</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4220</th>\n",
       "      <td>83 climb nyc august weather first day may unbe...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>user phrase global warm abandon favor climat c...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222</th>\n",
       "      <td>global warm tube parodi enjoy ipcc ocra url</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4223</th>\n",
       "      <td>one eye golfer dare tell global warm twenti fi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4224</th>\n",
       "      <td>man made global warm hair brain theori 4 scien...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TWEET  CROYANCE  CONFIENCE\n",
       "0     global warm report urg govern act brussel belg...         1     1.0000\n",
       "1                 fight poverti global warm africa link         1     1.0000\n",
       "2     carbon offset vatican forest fail reduc global...         1     0.8786\n",
       "3     carbon offset vatican forest fail reduc global...         1     1.0000\n",
       "4            uruguay tool need vulner climat chang link         1     0.8087\n",
       "...                                                 ...       ...        ...\n",
       "4220  83 climb nyc august weather first day may unbe...         1     1.0000\n",
       "4221  user phrase global warm abandon favor climat c...         1     1.0000\n",
       "4222        global warm tube parodi enjoy ipcc ocra url         0     0.6411\n",
       "4223  one eye golfer dare tell global warm twenti fi...         0     1.0000\n",
       "4224  man made global warm hair brain theori 4 scien...         0     1.0000\n",
       "\n",
       "[4225 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "messagesTwitter['TWEET'] = messagesTwitter['TWEET'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split() if word not in (stopword)]))\n",
    "messagesTwitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cf5ae8",
   "metadata": {},
   "source": [
    "<h2> Étape 4 : la lemmatisation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e2829ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e31a52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TWEET</th>\n",
       "      <th>CROYANCE</th>\n",
       "      <th>CONFIENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>global warm report urg govern act brussel belg...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fight poverti global warm africa link</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>carbon offset vatican forest fail reduc global...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>carbon offset vatican forest fail reduc global...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uruguay tool need vulner climat chang link</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4220</th>\n",
       "      <td>83 climb nyc august weather first day may unbe...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>user phrase global warm abandon favor climat c...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222</th>\n",
       "      <td>global warm tube parodi enjoy ipcc ocra url</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4223</th>\n",
       "      <td>one eye golfer dare tell global warm twenti fi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4224</th>\n",
       "      <td>man made global warm hair brain theori 4 scien...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TWEET  CROYANCE  CONFIENCE\n",
       "0     global warm report urg govern act brussel belg...         1     1.0000\n",
       "1                 fight poverti global warm africa link         1     1.0000\n",
       "2     carbon offset vatican forest fail reduc global...         1     0.8786\n",
       "3     carbon offset vatican forest fail reduc global...         1     1.0000\n",
       "4            uruguay tool need vulner climat chang link         1     0.8087\n",
       "...                                                 ...       ...        ...\n",
       "4220  83 climb nyc august weather first day may unbe...         1     1.0000\n",
       "4221  user phrase global warm abandon favor climat c...         1     1.0000\n",
       "4222        global warm tube parodi enjoy ipcc ocra url         0     0.6411\n",
       "4223  one eye golfer dare tell global warm twenti fi...         0     1.0000\n",
       "4224  man made global warm hair brain theori 4 scien...         0     1.0000\n",
       "\n",
       "[4225 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "messagesTwitter['TWEET'] = messagesTwitter['TWEET'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split(' ')]))\n",
    "messagesTwitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e82eb4",
   "metadata": {},
   "source": [
    "# phases d'apprentissage et de prédiction\n",
    "<h2>1. Découpage en jeux de tests et d’apprentissage</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "550d1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Création X,Y\n",
    "# X = messagesTwitter.iloc[:,[0,2]].values\n",
    "# Y = messagesTwitter.iloc[:,1].values\n",
    "# X_APPRENTISSAGE, X_VALIDATION, Y_APPRENTISSAGE, Y_VALIDATION = train_test_split(X, Y, test_size= 0.2, random_state= 0)\n",
    "\n",
    "\n",
    "# #Creation des datas d'apprentissage/validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(messagesTwitter['TWEET'].values, messagesTwitter['CROYANCE'].values, test_size= 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c182657",
   "metadata": {},
   "source": [
    "<h2>2. Création d’un pipeline d’apprentissage</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab98a68",
   "metadata": {},
   "source": [
    "La première action ajoutée au pipeline va consister, à l’aide de la fonction <b>CountVectorizer(),</b> à créer la matrice des occurrences des différents mots dans les différentes phrases comme nous avons appris à le faire manuellement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1163c63b",
   "metadata": {},
   "source": [
    "<b>(Terme Frequency )</b>, et à leur nombre d’apparitions dans l’ensemble des messages <b>(Inverse document frequency).</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32295eea",
   "metadata": {},
   "source": [
    "Ce poids appelé <b>TF-IDF </b>sert également à exprimer la rareté du mot dans l’ensemble des messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9de745",
   "metadata": {},
   "source": [
    "le TF-IDF décroît quand un mot est présent dans beaucoup de messages ;<br>\n",
    "\n",
    "le TF-IDF décroît également quand il est peu présent dans un message ;<br>\n",
    "\n",
    "le TF-IDF est maximal pour les mots peu fréquents apparaissant beaucoup dans l’ensemble des messages que nous avons à analyser.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e2790f",
   "metadata": {},
   "source": [
    "Le mot \"chien\" apparaît plusieurs fois dans les phrases, son TF-IDF sera faible.<br><br>\n",
    "\n",
    "Les mots \"bleue\" et \"rouge\" apparaissent peu de fois, leur TF-IDF sera faible également.<br><br>\n",
    "\n",
    "Le mot \"balle\" apparaît certes peu de fois dans les deux phrases, mais au moins une fois dans les deux. Sa fréquence (TF) est donc faible, mais il apparaît dans deux phrases (IDF). Son TF-IDF sera donc élevé.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf17df44",
   "metadata": {},
   "source": [
    "Ex: <br>\n",
    "Le chien joue dehors avec d’autres chiens avec une balle bleue.<br>\n",
    "Le chien et les autres chiens jouent dehors avec une balle rouge.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7fda83",
   "metadata": {},
   "source": [
    "Le mot \"chien\" apparaît plusieurs fois dans les phrases, son TF-IDF sera faible.<br><br>\n",
    "\n",
    "Les mots \"bleue\" et \"rouge\" apparaissent peu de fois, leur TF-IDF sera faible également.<br><br>\n",
    "\n",
    "Le mot \"balle\" apparaît certes peu de fois dans les deux phrases, mais au moins une fois dans les deux. Sa fréquence (TF) est donc faible, mais il apparaît dans deux phrases (IDF). Son TF-IDF sera donc élevé.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c81c8a0",
   "metadata": {},
   "source": [
    "<h1>L e TF_IDF est calculé à l'aide de la fonction TdiTransformer()du module Scikit-Learn</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6914fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB # l'algorithme Naive Byes\n",
    "\n",
    "etapes_apprentissage = Pipeline([('frequence',CountVectorizer()),('tfidf', TfidfTransformer()),('algorithme',MultinomialNB())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719df559",
   "metadata": {},
   "source": [
    "# 3. Apprentissage et analyse de resultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "534c85cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "modele = etapes_apprentissage.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c1e156c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8400    0.2739    0.4131       230\n",
      "           1     0.7831    0.9805    0.8708       615\n",
      "\n",
      "    accuracy                         0.7882       845\n",
      "   macro avg     0.8116    0.6272    0.6419       845\n",
      "weighted avg     0.7986    0.7882    0.7462       845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, modele.predict(X_test), digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82524189",
   "metadata": {},
   "source": [
    "# 4. Classification d'un nouveau message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9db4498d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/modele_twitter.mod']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.externals \n",
    "import joblib\n",
    "fichier = 'data/modele_twitter.mod'\n",
    "joblib.dump(modele, fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55f890e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "veuillez saisir votre message  Why should trust scientists with global warming if they didnt know Pluto wasnt a planet\n",
      "                                               TWEET\n",
      "0  trust scientists global warming didnt know plu...\n",
      ">> Croit au rechauffement climatique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "message = input(\"veuillez saisir votre message  \")\n",
    "messagesTwitter = pd.DataFrame(data=message, index=[0], columns=['TWEET'])\n",
    "\n",
    "\n",
    "# normalisation\n",
    "\n",
    "def normalisation(message):\n",
    "    message = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', message)\n",
    "    message = re.sub('@[^\\s]+','USER', message)\n",
    "    message = message.lower().replace(\"ё\", \"е\")\n",
    "    message = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', message)\n",
    "    message = re.sub(' +',' ', message)\n",
    "    return message.strip()\n",
    "\n",
    "messagesTwitter.TWEET= messagesTwitter.TWEET.apply(normalisation)\n",
    "messagesTwitter.head(3)\n",
    "\n",
    "# stopword \n",
    "stopword = stopwords.words('english')\n",
    "\n",
    "messagesTwitter['TWEET'] = messagesTwitter['TWEET'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopword)]))\n",
    "print(messagesTwitter)\n",
    "\n",
    "# stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "messagesTwitter['TWEET'] = messagesTwitter['TWEET'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split() if word not in (stopword)]))\n",
    "messagesTwitter\n",
    "\n",
    "# lemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "messagesTwitter['TWEET'] = messagesTwitter['TWEET'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split(' ')]))\n",
    "messagesTwitter\n",
    "\n",
    "# prediction\n",
    "\n",
    "prediction = modele.predict(messagesTwitter['TWEET'])\n",
    "\n",
    "if(prediction[0]==0):\n",
    "\n",
    "     print(\">> Ne croit pas au rechauffement climatique…\")\n",
    "\n",
    "else:\n",
    "\n",
    "     print(\">> Croit au rechauffement climatique...\")\n",
    "        \n",
    "        \n",
    "#message : Why should trust scientists with global warming if they didnt know Pluto wasnt a planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72c899db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>jamiemcintyre i d think cataclysmic might be a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>twenty years before the u s chamber of commerc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>in conclusion of the graphed data part of this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>bataille chris stephenleahy this is why i ve a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i think we should talk abt how the phrase clim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>330</td>\n",
       "      <td>randolphdogood1 hausfath nbcnews you know this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>331</td>\n",
       "      <td>colas2ok bruceanderson cathmckenna the ancient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>332</td>\n",
       "      <td>marknelsenkptv fox12oregon warmingclimate real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>333</td>\n",
       "      <td>jswatz xriskology michaelemann nytimes nytclim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>334</td>\n",
       "      <td>brian671 33 6 robotkiin1 it s called climate c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>335 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                              tweet\n",
       "0             0  jamiemcintyre i d think cataclysmic might be a...\n",
       "1             1  twenty years before the u s chamber of commerc...\n",
       "2             2  in conclusion of the graphed data part of this...\n",
       "3             3  bataille chris stephenleahy this is why i ve a...\n",
       "4             4  i think we should talk abt how the phrase clim...\n",
       "..          ...                                                ...\n",
       "330         330  randolphdogood1 hausfath nbcnews you know this...\n",
       "331         331  colas2ok bruceanderson cathmckenna the ancient...\n",
       "332         332  marknelsenkptv fox12oregon warmingclimate real...\n",
       "333         333  jswatz xriskology michaelemann nytimes nytclim...\n",
       "334         334  brian671 33 6 robotkiin1 it s called climate c...\n",
       "\n",
       "[335 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Twitter = pd.read_csv('data/tweet1.csv', sep=',')\n",
    "Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f37b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f0056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ca58f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textblob import TextBlob\n",
    "\n",
    "# def get_tweet_sentiment(self,tweet):\n",
    "#     analysis = TextBlob(self.clean_tweet(tweet))\n",
    "#     if analysis.sentiment.polarity > 0:\n",
    "#         return 'positive'\n",
    "#     elif analysis.sentiment.polarity == 0:\n",
    "#         return 'neutral'\n",
    "#     else:\n",
    "#         return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46d423b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def normalisation(message):\n",
    "#     message = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', message)\n",
    "#     message = re.sub('@[^\\s]+','USER', message)\n",
    "#     message = message.lower().replace(\"ё\", \"е\")\n",
    "#     message = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', message)\n",
    "#     message = re.sub(' +',' ', message)\n",
    "#     return message.strip()\n",
    "\n",
    "# Twitter.tweet= Twitter.tweet.apply(normalisation)\n",
    "# Twitter.head(3)\n",
    "\n",
    "# # stopword \n",
    "# stopword = stopwords.words('english')\n",
    "\n",
    "# Twitter['tweet'] = Twitter['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopword)]))\n",
    "# print(Twitter)\n",
    "\n",
    "# # stemmer\n",
    "# stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Twitter['tweet'] = Twitter['tweet'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split() if word not in (stopword)]))\n",
    "# Twitter\n",
    "\n",
    "# # lemmatizer\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# Twitter['tweet'] = Twitter['tweet'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split(' ')]))\n",
    "# Twitter\n",
    "\n",
    "# # prediction\n",
    "\n",
    "# prediction = modele.predict(Twitter['tweet'])\n",
    "\n",
    "# if(prediction[0]==0):\n",
    "\n",
    "#      print(\">> Ne croit pas au rechauffement climatique…\")\n",
    "\n",
    "# else:\n",
    "\n",
    "#      print(\">> Croit au rechauffement climatique...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1180fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3093a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
